{
  "hash": "916444ebbea98273cdb72873b9a51de9",
  "result": {
    "markdown": "---\ntitle: \"CLUSTERING\"\nauthor: \"Ronit Baishya\"\ndate: \"2023-12-08\"\ncategories: [Clustering]\nimage: \"Photo.png\"\n---\n\n## What is Probability?\n\nProbability is a measure quantifying the likelihood that a specific event will occur. It is a number between 0 and 1, where 0 represents the impossibility of the event happening, and 1 represents the certainty of the event occurring. Intermediate values between 0 and 1 indicate degrees of likelihood. Key concepts related to probability include sample space, event, probability of an even, complement of an event, mutually exclusive event and independent event.\n\n## What is Random Variable?\n\nA random variable is a mathematical concept used in probability theory and statistics to describe the outcomes of a random process or experiment. It assigns a numerical value to each possible outcome of an experiment, and its values are determined by chance.\n\nThere are two types of random variables:\n\n1.  **Discrete Random Variable:**\n\n    -   Takes on a countable number of distinct values.\n\n    -   Often associated with experiments that involve counting or finite outcomes.\n\n    -   Examples include the number of heads in multiple coin tosses, the number of cars passing through an intersection in a given time period, or the outcome of rolling a die.\n\n2.  **Continuous Random Variable:**\n\n    -   Can take on an infinite number of possible values within a given range.\n\n    -   Often associated with measurements or observations that can take any value within a range.\n\n    -   Examples include the height of a person, the time it takes for a process to complete, or the temperature in a given location.\n\n## Connection Between Probability and Random Variable\n\nProbability and random variables are closely connected concepts in probability theory and statistics. A random variable is a mathematical function that assigns a numerical value to each possible outcome of a random experiment, while probability measures the likelihood or chance associated with each of these outcomes. Events involving random variables are defined based on the outcomes they map to the probability of an event involving a random variable is calculated by summing (for discrete random variables) or integrating (for continuous random variables) the probabilities associated with the outcomes in the event. The expected value (mean) of a random variable is a measure of central tendency and is calculated as the weighted average of all possible values, where the weights are given by the probabilities. The variance and standard deviation of a random variable quantify the spread or variability of its values around the mean. Probability and random variables are fundamental concepts in probability theory and statistics, and their interplay allows for the formal modeling of uncertainty, the calculation of expected values, and the analysis of random phenomena in various fields.\n\n## Why We Need Probability and Random Variable in Machine Learning.\n\nProbability and random variables play crucial roles in machine learning. Understanding these concepts is essential for building, training, and evaluating machine learning models. Here are some key ways in which probability and random variables are employed in machine learning:\n\n### **1. Probabilistic Models:**\n\n-   Many machine learning models are based on probabilistic frameworks. For example, Bayesian models explicitly use probability distributions to model uncertainty in parameters and predictions.\n\n### **2. Uncertainty Modeling:**\n\n-   Probability theory provides a natural way to represent and quantify uncertainty. In machine learning, uncertainty arises due to various factors, such as noisy data, limited information, or inherent variability. Probabilistic models help in capturing and managing this uncertainty.\n\n### **3. Bayesian Inference:**\n\n-   Bayesian methods use probability theory to update beliefs about parameters or hypotheses based on observed data. Bayesian inference is used for parameter estimation, model selection, and making predictions in a probabilistic manner.\n\n### **4. Classification and Prediction:**\n\n-   Many classification algorithms, such as Naive Bayes and logistic regression, are inherently probabilistic. They provide probabilities associated with class assignments, allowing for a probabilistic interpretation of predictions.\n\n### **5. Regression Analysis:**\n\n-   In regression tasks, probability distributions can be used to model the uncertainty in predictions. Bayesian linear regression, for example, provides a distribution over possible regression lines rather than a single point estimate.\n\n### **6. Ensemble Methods:**\n\n-   Ensemble methods, such as Random Forests and Gradient Boosting, often use randomness in the form of random sampling or random feature selection to improve model performance and generalization.\n\n### **7. Monte Carlo Methods:**\n\n-   Monte Carlo methods, which rely on random sampling, are used for numerical integration, optimization, and simulation in machine learning. These methods are foundational in Bayesian statistics and probabilistic graphical models.\n\n### **8. Deep Learning:**\n\n-   In deep learning, stochastic gradient descent (SGD) is a popular optimization algorithm that uses randomness in selecting batches of training data to update model parameters. Dropout, a regularization technique, also introduces randomness during training.\n\n## 1. Finding The Probability Density and Z-Score\n\nLet's use an example with the well-known Iris dataset, which is often used in machine learning. We'll create a probability distribution plot (histogram) for one of the features in the Iris dataset using Python and the Seaborn library. Seaborn is built on top of Matplotlib and provides a high-level interface for drawing attractive statistical graphics. Then also plot the Z-score, also known as the standard score, is a measure of how many standard deviations a data point is from the mean of a dataset. It is a dimensionless quantity and is expressed in terms of standard deviations.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\ndata = iris.data\nfeature_names = iris.feature_names\n\n# Select a feature for the probability distribution plot and Z-score calculation (e.g., sepal length)\nselected_feature_index = 0\nselected_feature_name = feature_names[selected_feature_index]\nselected_feature_data = data[:, selected_feature_index]\n\n# Calculate the Z-score\nmean_value = np.mean(selected_feature_data)\nstd_dev = np.std(selected_feature_data)\nz_scores = (selected_feature_data - mean_value) / std_dev\n\n# Create a probability distribution plot (histogram) with Z-score markers\nplt.figure(figsize=(10, 6))\n\n# Probability Distribution Plot\nsns.histplot(selected_feature_data, kde=True, color='blue', bins=20, label='Probability Distribution')\n\n# Z-score Markers\nplt.axvline(mean_value, color='red', linestyle='dashed', linewidth=2, label='Mean')\nplt.axvline(mean_value + std_dev, color='green', linestyle='dashed', linewidth=2, label='Mean + 1 Std Dev')\nplt.axvline(mean_value - std_dev, color='green', linestyle='dashed', linewidth=2, label='Mean - 1 Std Dev')\n\nplt.title(f'Probability Distribution Plot and Z-scores of {selected_feature_name}')\nplt.xlabel(selected_feature_name)\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()\n\n# Display the Z-scores\nprint(f'Z-scores for {selected_feature_name}:\\n{z_scores}')\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=808 height=523}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nZ-scores for sepal length (cm):\n[-0.90068117 -1.14301691 -1.38535265 -1.50652052 -1.02184904 -0.53717756\n -1.50652052 -1.02184904 -1.74885626 -1.14301691 -0.53717756 -1.26418478\n -1.26418478 -1.87002413 -0.05250608 -0.17367395 -0.53717756 -0.90068117\n -0.17367395 -0.90068117 -0.53717756 -0.90068117 -1.50652052 -0.90068117\n -1.26418478 -1.02184904 -1.02184904 -0.7795133  -0.7795133  -1.38535265\n -1.26418478 -0.53717756 -0.7795133  -0.41600969 -1.14301691 -1.02184904\n -0.41600969 -1.14301691 -1.74885626 -0.90068117 -1.02184904 -1.62768839\n -1.74885626 -1.02184904 -0.90068117 -1.26418478 -0.90068117 -1.50652052\n -0.65834543 -1.02184904  1.40150837  0.67450115  1.2803405  -0.41600969\n  0.79566902 -0.17367395  0.55333328 -1.14301691  0.91683689 -0.7795133\n -1.02184904  0.06866179  0.18982966  0.31099753 -0.29484182  1.03800476\n -0.29484182 -0.05250608  0.4321654  -0.29484182  0.06866179  0.31099753\n  0.55333328  0.31099753  0.67450115  0.91683689  1.15917263  1.03800476\n  0.18982966 -0.17367395 -0.41600969 -0.41600969 -0.05250608  0.18982966\n -0.53717756  0.18982966  1.03800476  0.55333328 -0.29484182 -0.41600969\n -0.41600969  0.31099753 -0.05250608 -1.02184904 -0.29484182 -0.17367395\n -0.17367395  0.4321654  -0.90068117 -0.17367395  0.55333328 -0.05250608\n  1.52267624  0.55333328  0.79566902  2.12851559 -1.14301691  1.76501198\n  1.03800476  1.64384411  0.79566902  0.67450115  1.15917263 -0.17367395\n -0.05250608  0.67450115  0.79566902  2.24968346  2.24968346  0.18982966\n  1.2803405  -0.29484182  2.24968346  0.55333328  1.03800476  1.64384411\n  0.4321654   0.31099753  0.67450115  1.64384411  1.88617985  2.4920192\n  0.67450115  0.55333328  0.31099753  2.24968346  0.55333328  0.67450115\n  0.18982966  1.2803405   1.03800476  1.2803405  -0.05250608  1.15917263\n  1.03800476  1.03800476  0.55333328  0.79566902  0.4321654   0.06866179]\n```\n:::\n:::\n\n\n## 2. Conditional Probability\n\nTo illustrate conditional probability using the Iris dataset, let's create a scatter plot that shows the relationship between two features (e.g., sepal length and sepal width) for a specific class of flowers. We'll use the Seaborn library for data visualization. Below is an example code that generates a scatter plot for sepal length vs. sepal width, conditioned on the Iris class.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\ndata = iris.data\ntarget_names = iris.target_names\n\n# Create a DataFrame for easy plotting using Seaborn\nimport pandas as pd\ndf = pd.DataFrame(data, columns=iris.feature_names)\ndf['class'] = [target_names[i] for i in iris.target]\n\n# Choose two features for the scatter plot (e.g., sepal length and sepal width)\nfeature1 = 'sepal length (cm)'\nfeature2 = 'sepal width (cm)'\n\n# Set the style of the plot\nsns.set(style=\"whitegrid\")\n\n# Create a scatter plot conditioned on the Iris class\nplt.figure(figsize=(10, 8))\nsns.scatterplot(x=feature1, y=feature2, hue='class', data=df, palette='viridis', s=100)\n\n# Set plot labels and title\nplt.title(f'Scatter Plot of {feature1} vs. {feature2} (Conditional on Iris Class)')\nplt.xlabel(feature1)\nplt.ylabel(feature2)\n\n# Show the legend\nplt.legend(title='Iris Class')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=816 height=677}\n:::\n:::\n\n\nIn this example, the scatter plot visualizes the relationship between sepal length and sepal width for each Iris class. Each point represents an individual flower, and the points are color-coded based on their class. This provides a conditional view of the data, showing how the two features vary within each class.\n\nYou can customize the code by changing the values of **`feature1`** and **`feature2`** to explore other combinations of features in the dataset. Adjust the Seaborn settings and plot parameters according to your preferences.\n\n## 3. Central Limit Theorem\n\nThe Central Limit Theorem (CLT) states that, under certain conditions, the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal (Gaussian) distribution, regardless of the original distribution. This theorem has significant implications for statistical inference. In the context of the Iris dataset example, we can demonstrate the Central Limit Theorem by considering the distribution of sample means for a specific feature. Let's create a plot that illustrates the CLT using the sepal length feature.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\nsepal_length = iris.data[:, 0]\n\n# Number of samples to draw for each mean\nsample_size = 30\n\n# Number of means to calculate\nnum_means = 1000\n\n# Generate means from random samples\nmeans = [np.mean(np.random.choice(sepal_length, size=sample_size)) for _ in range(num_means)]\n\n# Set the style of the plot\nsns.set(style=\"whitegrid\")\n\n# Create a histogram of sample means\nplt.figure(figsize=(10, 6))\nsns.histplot(means, kde=True, color='blue', bins=30)\n\n# Set plot labels and title\nplt.title('Distribution of Sample Means (Central Limit Theorem)')\nplt.xlabel('Sample Mean (Sepal Length)')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=812 height=529}\n:::\n:::\n\n\nIn this example, we randomly draw samples of sepal length and calculate the means of these samples. The resulting distribution of sample means is then visualized using a histogram. According to the Central Limit Theorem, this distribution of sample means should approach a normal distribution, regardless of the original distribution of sepal length.\n\nYou can experiment with different sample sizes (**`sample_size`**) and the number of means to calculate (**`num_means`**) to observe how the distribution of sample means becomes more normal as the sample size increases, in accordance with the Central Limit Theorem.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}