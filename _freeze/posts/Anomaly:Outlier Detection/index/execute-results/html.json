{
  "hash": "280a61612bcfe9de35888e65659d8cb0",
  "result": {
    "markdown": "---\ntitle: \"ANOMALY/OUTLIER DETECTION\"\nauthor: \"Ronit Baishya\"\ndate: \"2023-12-08\"\ncategories: [Anomaly or Outlier Detection]\nimage: \"Photo.png\"\n---\n\n## What is Anomaly/Outlier Detection?\n\nAnomaly detection, also known as outlier detection, is a technique in data analysis and machine learning that focuses on identifying patterns or instances that deviate significantly from the majority of the data. These anomalies are observations that do not conform to expected behavior and may indicate unusual or potentially fraudulent activities, errors, or rare events.\n\nKey points about anomaly detection:\n\n1.  **Unsupervised Learning:**\n\n    -   Anomaly detection is often implemented as an unsupervised learning task, meaning that the algorithm identifies anomalies without relying on labeled data explicitly indicating which instances are anomalous.\n\n2.  **Types of Anomalies:**\n\n    -   Anomalies can take various forms, including outliers, novelties, or deviations from established patterns. Outliers are observations that are significantly different from the majority, while novelties refer to previously unseen patterns.\n\n3.  **Applications:**\n\n    -   Anomaly detection is applied in various domains, such as fraud detection in financial transactions, network security monitoring, manufacturing quality control, healthcare monitoring, and predictive maintenance in machinery.\n\n4.  **Techniques:**\n\n    -   There are several techniques for anomaly detection, including statistical methods, machine learning algorithms, and deep learning approaches. Common statistical methods include z-score, Mahalanobis distance, and isolation forests. Machine learning algorithms such as one-class SVM (Support Vector Machines), k-nearest neighbors (KNN), and autoencoders are frequently used.\n\n5.  **Threshold-based Approaches:**\n\n    -   One common approach in anomaly detection is setting a threshold beyond which data points are considered anomalous. Data points with a measure (e.g., distance, error) exceeding this threshold are flagged as anomalies.\n\n6.  **Data Preprocessing:**\n\n    -   Proper data preprocessing is crucial for effective anomaly detection. This may involve scaling, transforming, or handling missing values in the data.\n\n7.  **Evaluation Metrics:**\n\n    -   Anomaly detection performance is evaluated using metrics such as precision, recall, F1-score, and area under the receiver operating characteristic (ROC) curve.\n\n8.  **Challenges:**\n\n    -   Anomaly detection faces challenges, such as defining what constitutes normal behavior, handling imbalanced datasets, and adapting to evolving patterns over time.\n\nAnomaly detection plays a crucial role in ensuring the integrity and security of systems, identifying potential issues, and enabling proactive measures to address abnormal situations. The choice of a specific anomaly detection technique depends on the characteristics of the data and the requirements of the application.\n\n## Visualization\n\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique that can be used for anomaly detection by identifying patterns and reducing the feature space. In this example, we'll use PCA to reduce the dimensionality of the Iris dataset and then apply an anomaly detection approach based on the reconstruction error. The idea is that anomalies might result in higher reconstruction errors when transforming and then inversely transforming the data using the principal components. In this code:\n\n-   The Iris dataset is standardized using **`StandardScaler`** to ensure that each feature has a mean of 0 and a standard deviation of 1.\n\n-   PCA is applied to reduce the dimensionality to two principal components.\n\n-   An Isolation Forest model is then applied to the reduced data for anomaly detection.\n\n-   Anomalies are identified based on the Isolation Forest predictions, and the anomalies are plotted in red.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Standardize the features\nscaler = StandardScaler()\nX_standardized = scaler.fit_transform(X)\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_standardized)\n\n# Apply Isolation Forest for anomaly detection on the reduced data\nclf = IsolationForest(contamination=0.1, random_state=42)\nclf.fit(X_pca)\n\n# Get anomaly scores\nanomaly_scores = clf.decision_function(X_pca)\n\n# Predict the anomalies\ny_pred = clf.predict(X_pca)\nanomaly_mask = y_pred == -1  # Anomalies are marked as -1\n\n# Plot the original data and highlight anomalies with connecting lines\nplt.figure(figsize=(12, 6))\n\n# Scatter plot\nplt.subplot(1, 2, 1)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c='blue', label='Normal')\nplt.scatter(X_pca[anomaly_mask, 0], X_pca[anomaly_mask, 1], c='red', label='Anomaly')\n\n# Connect the anomaly points with a line\nanomaly_points = X_pca[anomaly_mask]\nfor i in range(1, len(anomaly_points)):\n    plt.plot([anomaly_points[i-1, 0], anomaly_points[i, 0]],\n             [anomaly_points[i-1, 1], anomaly_points[i, 1]],\n             c='red', linestyle='--')\n\nplt.title('Anomaly Detection using PCA')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\n\n# Anomaly score plot\nplt.subplot(1, 2, 2)\nplt.scatter(range(len(anomaly_scores)), anomaly_scores, c='red', label='Anomaly Scores')\nplt.axhline(y=0, color='blue', linestyle='--', label='Threshold')\nplt.title('Anomaly Scores')\nplt.xlabel('Data Point Index')\nplt.ylabel('Anomaly Score')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=1141 height=564}\n:::\n:::\n\n\nTo get the anomaly scores assigned by the Isolation Forest model to each data point, you can use the **`decision_function`** method. The anomaly score represents the model's confidence that a data point is an anomaly, and lower scores generally indicate a higher likelihood of being an anomaly. The **`decision_function`** method is used to obtain the anomaly scores. The anomaly scores are then plotted, and a threshold (horizontal dashed line) is included to help visualize the separation between normal and anomalous points. Adjust the threshold based on your specific requirements and the characteristics of the dataset.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Choose the feature for anomaly detection (e.g., Petal Length)\nfeature_index = 2  # Petal Length\nX_feature = X[:, feature_index].reshape(-1, 1)\n\n# Standardize the feature\nscaler = StandardScaler()\nX_feature_standardized = scaler.fit_transform(X_feature)\n\n# Apply Local Outlier Factor (LOF) for anomaly detection\nlof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\nlof.fit_predict(X_feature_standardized)\nanomaly_scores_lof = -lof.negative_outlier_factor_\n\n# Apply Isolation Forest for anomaly detection\nisolation_forest = IsolationForest(contamination=0.1, random_state=42)\nisolation_forest.fit(X_feature_standardized)\nanomaly_scores_isolation_forest = isolation_forest.decision_function(X_feature_standardized)\n\n# Plot the anomaly scores for both methods\nplt.figure(figsize=(12, 6))\n\n# Anomaly scores using Local Outlier Factor (LOF)\nplt.subplot(1, 2, 1)\nplt.scatter(X_feature, anomaly_scores_lof, c='red', label='Anomaly Scores (LOF)')\nplt.axhline(y=0, color='blue', linestyle='--', label='Threshold')\nplt.title('Anomaly Detection using LOF')\nplt.xlabel('Petal Length')\nplt.ylabel('Anomaly Score (LOF)')\nplt.legend()\n\n# Anomaly scores using Isolation Forest\nplt.subplot(1, 2, 2)\nplt.scatter(X_feature, anomaly_scores_isolation_forest, c='orange', label='Anomaly Scores (Isolation Forest)')\nplt.axhline(y=0, color='blue', linestyle='--', label='Threshold')\nplt.title('Anomaly Detection using Isolation Forest')\nplt.xlabel('Petal Length')\nplt.ylabel('Anomaly Score (Isolation Forest)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1141 height=564}\n:::\n:::\n\n\nIn the comparison, both plots show the anomaly scores for the \"Petal Length\" feature using LOF and Isolation Forest. The threshold is visualized with a horizontal dashed line. You can observe how each method identifies anomalies based on the anomaly scores. Adjust the parameters of the algorithms and explore different features for further experimentation. The left subplot shows the anomaly scores and the threshold for the Local Outlier Factor (LOF). The left subplot shows the anomaly scores and the threshold for the Local Outlier Factor (LOF). The right subplot shows the anomaly scores and the threshold for the Isolation Forest. Feel free to analyze the visual comparison and numerical scores to understand how each method identifies anomalies in the \"Petal Length\" feature. Adjust parameters and thresholds based on your specific requirements.\n\nIn practice, it's often a good idea to try multiple anomaly detection methods and compare their performance on your specific dataset. Additionally, consider the trade-offs between false positives and false negatives based on the consequences of missing true anomalies or incorrectly flagging normal instances as anomalies. In the provided example, you used both Local Outlier Factor (LOF) and Isolation Forest for anomaly detection. Visualize the results, adjust parameters, and assess the performance based on both visual inspection and quantitative metrics.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}