{
  "hash": "ddbbc6085b1e9c732c76f35d3cd6ede9",
  "result": {
    "markdown": "---\ntitle: \"LINEAR AND NON-LINEAR REGRESSION\"\nauthor: \"Ronit Baishya\"\ndate: \"2023-12-08\"\ncategories: [Linear And Non-Linear Regression]\nimage: \"Photo.png\"\n---\n\n## What is Regression?\n\nRegression is a statistical method used in machine learning and statistics to model the relationship between a dependent variable (or target) and one or more independent variables (or features). The goal of regression analysis is to understand the nature of the relationship and use this understanding to make predictions or infer the impact of changes in the independent variables on the dependent variable.\n\n## Linear Regression.\n\nLinear regression is a fundamental statistical and machine learning technique used to model the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data. The relationship is assumed to be approximately linear, meaning that a change in the independent variable(s) is associated with a constant change in the dependent variable. Linear regression aims to find the best-fitting straight line (hyperplane in the case of multiple variables) through the data points. Here's a Python code snippet that demonstrates how to compare two linear regression models using the Iris dataset:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Choose two features for the linear regression models\nfeature1 = 0  # Sepal length\nfeature2 = 2  # Petal length\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X[:, [feature1]], y, test_size=0.2, random_state=42)\n\n# Fit the first linear regression model\nmodel1 = LinearRegression()\nmodel1.fit(X_train, y_train)\n\n# Predict the target variable for the test set\ny_pred1 = model1.predict(X_test)\n\n# Calculate mean squared error and R-squared for the first model\nmse1 = mean_squared_error(y_test, y_pred1)\nr2_1 = r2_score(y_test, y_pred1)\n\n# Fit the second linear regression model using a different feature\nX_train, X_test, y_train, y_test = train_test_split(X[:, [feature2]], y, test_size=0.2, random_state=42)\nmodel2 = LinearRegression()\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\nmse2 = mean_squared_error(y_test, y_pred2)\nr2_2 = r2_score(y_test, y_pred2)\n\n# Display the results\nprint(f\"Model 1 (Feature {feature1}):\")\nprint(f\"Mean Squared Error: {mse1}\")\nprint(f\"R-squared: {r2_1}\")\nprint()\nprint(f\"Model 2 (Feature {feature2}):\")\nprint(f\"Mean Squared Error: {mse2}\")\nprint(f\"R-squared: {r2_2}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel 1 (Feature 0):\nMean Squared Error: 0.1979546681395589\nR-squared: 0.7167580265093751\n\nModel 2 (Feature 2):\nMean Squared Error: 0.06351735484715113\nR-squared: 0.9091166623808649\n```\n:::\n:::\n\n\nIn this example, two linear regression models are trained and evaluated using different features (Sepal length and Petal length). The mean squared error (MSE) and R-squared values are calculated for each model. Keep in mind that linear regression may not be the best model for the Iris dataset due to its categorical target variable. Models like logistic regression or support vector machines are more appropriate for classification tasks.\n\n## Non Linear Regression\n\nFor non-linear regression, let's use the Support Vector Regression (SVR) model, which is capable of capturing non-linear relationships in the data. We'll again use two different features for the comparison. Here's the Python code snippet:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Choose two features for the non-linear regression models\nfeature1 = 0  # Sepal length\nfeature2 = 2  # Petal length\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X[:, [feature1]], y, test_size=0.2, random_state=42)\n\n# Fit the first non-linear regression model (SVR) using a radial basis function (RBF) kernel\nmodel1 = SVR(kernel='rbf')\nmodel1.fit(X_train, y_train)\n\n# Predict the target variable for the test set\ny_pred1 = model1.predict(X_test)\n\n# Calculate mean squared error and R-squared for the first model\nmse1 = mean_squared_error(y_test, y_pred1)\nr2_1 = r2_score(y_test, y_pred1)\n\n# Fit the second non-linear regression model using a different feature\nX_train, X_test, y_train, y_test = train_test_split(X[:, [feature2]], y, test_size=0.2, random_state=42)\nmodel2 = SVR(kernel='rbf')\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\nmse2 = mean_squared_error(y_test, y_pred2)\nr2_2 = r2_score(y_test, y_pred2)\n\n# Display the results\nprint(f\"Non-linear Model 1 (Feature {feature1}):\")\nprint(f\"Mean Squared Error: {mse1}\")\nprint(f\"R-squared: {r2_1}\")\nprint()\nprint(f\"Non-linear Model 2 (Feature {feature2}):\")\nprint(f\"Mean Squared Error: {mse2}\")\nprint(f\"R-squared: {r2_2}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-linear Model 1 (Feature 0):\nMean Squared Error: 0.17987340568021976\nR-squared: 0.7426294672302102\n\nNon-linear Model 2 (Feature 2):\nMean Squared Error: 0.035468618773986264\nR-squared: 0.9492499890356317\n```\n:::\n:::\n\n\nIn this example, two non-linear regression models (SVR with RBF kernel) are trained and evaluated using different features (Sepal length and Petal length). The mean squared error (MSE) and R-squared values are calculated for each model. The SVR model is particularly useful for capturing non-linear patterns in the data.\n\nComparing linear and non-linear regression models on the Iris dataset using different features, we can observe the performance of these models based on mean squared error (MSE) and R-squared values. Here's a summary of the results:\n\n## Comparison between model the Regression Methods\n\n### **1.  Linear Regression Models:**\n\n#### Model 1 (Feature: Sepal Length)\n\n-   Mean Squared Error (MSE): Varies depending on the split.\n\n-   R-squared: Varies depending on the split.\n\n#### Model 2 (Feature: Petal Length)\n\n-   Mean Squared Error (MSE): Varies depending on the split.\n\n-   R-squared: Varies depending on the split.\n\n### **2.  Non-linear Regression Models (SVR with RBF Kernel):**\n\n#### Model 1 (Feature: Sepal Length)\n\n-   Mean Squared Error (MSE): Varies depending on the split.\n\n-   R-squared: Varies depending on the split.\n\n#### Model 2 (Feature: Petal Length)\n\n-   Mean Squared Error (MSE): Varies depending on the split.\n\n-   R-squared: Varies depending on the split.\n\n### **Conclusion:**\n\nwe can plot the regression line along with the scatter points. For linear regression, the trend line is the line of best fit, and for non-linear regression, we can plot the predicted values against the feature values. Here's an updated version of the code with trend lines added. In this code, **`sns.regplot`** is used to plot the trend line along with the scatter points. Adjust the feature index (**`feature`**) and model parameters as needed for further exploration. We should keep in mind that while trend lines are shown for visualization purposes, their interpretation may vary based on the nature of the underlying models.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Choose a feature for the visualization\nfeature = 2  # Petal length\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X[:, [feature]], y, test_size=0.2, random_state=42)\n\n# Linear Regression Model\nmodel_linear = LinearRegression()\nmodel_linear.fit(X_train, y_train)\ny_pred_linear = model_linear.predict(X_test)\n\n# Non-linear Regression Model (SVR with RBF Kernel)\nmodel_nonlinear = SVR(kernel='rbf')\nmodel_nonlinear.fit(X_train, y_train)\ny_pred_nonlinear = model_nonlinear.predict(X_test)\n\n# Scatter plot for Linear Regression Model with Trend Line\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.scatterplot(x=X_test[:, 0], y=y_test, label='True Values', color='blue')\nsns.regplot(x=X_test[:, 0], y=y_pred_linear, scatter=False, label='Trend Line', color='orange')\nplt.title('Linear Regression Model')\nplt.xlabel('Petal Length')\nplt.ylabel('Target Variable')\nplt.legend()\n\n# Scatter plot for Non-linear Regression Model with Trend Line\nplt.subplot(1, 2, 2)\nsns.scatterplot(x=X_test[:, 0], y=y_test, label='True Values', color='blue')\nsns.regplot(x=X_test[:, 0], y=y_pred_nonlinear, scatter=False, label='Trend Line', color='green')\nplt.title('Non-linear Regression Model (SVR with RBF Kernel)')\nplt.xlabel('Petal Length')\nplt.ylabel('Target Variable')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=1141 height=564}\n:::\n:::\n\n\n-   The choice between linear and non-linear regression models depends on the underlying relationship between the features and the target variable.\n\n-   For the Iris dataset, which has a categorical target variable representing different species of flowers, regression may not be the most suitable modeling approach. Classification models like logistic regression or support vector machines are more appropriate for predicting categorical outcomes.\n\n-   If the goal is to model a continuous target variable in a non-linear fashion, non-linear regression models such as SVR with an RBF kernel may capture more complex relationships.\n\n-   Evaluation metrics such as MSE and are important for assessing model performance, but the interpretation may be limited due to the nature of the dataset and the choice of regression models.\n\n-   It's crucial to choose the right model based on the characteristics of the data and the task at hand. Consideration of other modeling techniques and appropriate preprocessing steps may be necessary for more meaningful and accurate predictions.\n\nWe should keep in mind that the Iris dataset is not inherently suitable for regression analysis, and the primary goal of this comparison is for illustrative purposes. For regression tasks, datasets with continuous target variables are typically more appropriate.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}