{
  "hash": "4d6f8088267c92d26073b07ce69ae035",
  "result": {
    "markdown": "---\ntitle: \"CLUSTERING\"\nauthor: \"Ronit Baishya\"\ndate: \"2023-12-08\"\ncategories: [Clustering]\nimage: \"Photo.png\"\n---\n\n## What is Clustering?\n\nClustering is a technique in machine learning and data analysis that involves grouping similar data points together based on certain criteria. The goal of clustering is to identify inherent structures within a dataset and organize data into groups, or clusters, such that points within the same cluster are more similar to each other than they are to points in other clusters.\n\n## Key characteristics of Clustering:\n\n1.  **Unsupervised Learning:**\n\n    -   Clustering is often considered an unsupervised learning task because the algorithm works without labeled target values. The goal is to discover patterns or structures in the data without explicit guidance on what those patterns might be.\n\n2.  **Similarity Measures:**\n\n    -   Clustering relies on similarity or dissimilarity measures to determine how close or far apart data points are from each other. Common measures include Euclidean distance, Manhattan distance, or other distance metrics depending on the type of data and the clustering algorithm used.\n\n3.  **Cluster Centroids:**\n\n    -   Clusters are often represented by a central point, known as the centroid. The centroid can be the mean, median, or another representative point for the data in the cluster.\n\n4.  **Types of Clustering:**\n\n    -   There are various types of clustering algorithms, and they can be broadly categorized into hierarchical clustering and partitional clustering. Partitional clustering methods, such as K-means, divide the data into non-overlapping clusters. Hierarchical clustering methods, such as agglomerative clustering, build a hierarchy of clusters.\n\n5.  **Applications:**\n\n    -   Clustering is used in a variety of fields, including data analysis, image segmentation, customer segmentation in marketing, anomaly detection, document categorization, and more. It is also used as a preprocessing step for other machine learning tasks.\n\n6.  **Evaluation Metrics:**\n\n    -   The effectiveness of clustering algorithms can be assessed using metrics such as silhouette score, Davies-Bouldin index, or other measures that evaluate the compactness and separation of clusters.\n\n## Popular clustering algorithms include:\n\n-   **K-means Clustering:** Divides data into K clusters based on centroids.\n\n-   **Hierarchical Clustering:** Builds a hierarchy of clusters by iteratively merging or splitting clusters.\n\n-   **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** Identifies clusters based on density and can find clusters of irregular shapes.\n\n-   **Agglomerative Clustering:** A hierarchical clustering approach that starts with individual data points as clusters and merges them until a stopping criterion is met.\n\nClustering is a powerful tool for exploring patterns and structures within data, facilitating better understanding and decision-making in various domains. The choice of a clustering algorithm depends on the nature of the data and the goals of the analysis.\n\n## Example\n\nLet's use the K-means clustering algorithm to cluster the Iris dataset based on its features and visualize the clusters. We'll use the Seaborn and Matplotlib libraries for data visualization and the scikit-learn library for the K-means clustering algorithm.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\nfeature_names = iris.feature_names\n\n# Number of clusters (you can adjust this based on your needs)\nnum_clusters = 3\n\n# Apply K-means clustering\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\ncluster_labels = kmeans.fit_predict(X)\n\n# Add cluster labels to the dataset\niris_df = sns.load_dataset(\"iris\")\niris_df['cluster'] = cluster_labels\n\n# Set the style of the plot\nsns.set(style=\"whitegrid\")\n\n# Create a pairplot with clusters colored differently\nplt.figure(figsize=(12, 8))\nsns.pairplot(iris_df, hue=\"cluster\", palette=\"viridis\", markers=[\"o\", \"s\", \"D\"])\n\n# Set plot title\nplt.suptitle(f'Pairplot of Iris Dataset with {num_clusters} Clusters', y=1.02)\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/ronitbaishya/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/ronitbaishya/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 1152x768 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-3.png){width=1014 height=980}\n:::\n:::\n\n\nIn this example, we're using a pairplot to visualize the relationships between different pairs of features in the Iris dataset. Each point in the scatter plots is color-coded based on the cluster assigned by the K-means algorithm. You can experiment with the number of clusters (**`num_clusters`**) to see how the data is partitioned. Adjust the **`markers`** parameter in the **`sns.pairplot`** function to use different markers for each cluster. The goal is to observe how well K-means separates the data into distinct clusters based on the chosen features.\n\nEvaluating the performance of clustering algorithms can be a bit challenging, especially in the absence of ground truth labels. However, there are several metrics and techniques that can provide insights into the quality of the clustering results. Here are a few common methods:\n\n### **Davies-Bouldin Index:**\n\nThe Davies-Bouldin index measures the compactness and separation between clusters. Lower values indicate better clustering.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.metrics import davies_bouldin_score\n\ndb_index = davies_bouldin_score(X, cluster_labels)\nprint(f\"Davies-Bouldin Index: {db_index}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDavies-Bouldin Index: 0.6619715465007465\n```\n:::\n:::\n\n\nIt's important to note that evaluation metrics depend on the characteristics of the data and the goals of clustering. In some cases, visual inspection and domain knowledge may be more informative than numerical metrics. Additionally, the choice of the number of clusters (**`num_clusters`**) can affect the results, and it may be worthwhile to experiment with different values. We also have to keep in mind that the Iris dataset used here has ground truth labels (flower species), but in a real-world scenario without such labels, evaluation becomes more challenging and subjective.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}